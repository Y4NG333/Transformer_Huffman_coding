1.Created a new branch in the local repository and the remote repository via the git command line 

2.tgt_vocab_size changed from 26 to 3（0,1, and padding ）

3.The corresponding attention map is generated by the dec_enc_attens parameter, but the result is quite different from the ideal state. 
![_GDV)QQX92LKD 7C4TJ6ZZR](https://user-images.githubusercontent.com/91429283/157084321-51699496-f9d4-4b66-8e7b-0181c226bf11.png)

（Since PADDING is represented by 0, the number 2 is used to represent 0. ）

When studying the translation results, a pattern was found:
The recognition effect of the model for padding is very poor. For example, if the huffman coding corresponding to a is (1,1,1,1,PADDING) five digits, the translation result given by the final model often does not set the last digit to PADDING, but randomly selects one of 1 and 0 to fill in.
In other words, iterative learning did not make the model learn to fill in PADDING where needed .

