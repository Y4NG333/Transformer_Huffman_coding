The main task of this PR is to add the inference function to the test file, but there were a lot of problems during the addition process. 

After a long time to solve them all, the attention map is almost the same as we expected.

The first problem：
After comparing with other Transformer models, my model lacks a begin symbol in the decoder part. For example: in my model, the huffman encoding corresponding to abacc is: 01101010, while other models have a begin symbol: corresponding to abacc The huffman encoding is: (begin) 01101010

The second problem：
After adding the begin symbol, assuming that the begin symbol is 3, the previous training does not make the inference function appear the expected sequence, for example, abcab should appear: 301110011, but the result is 333333333.So in the Loss part, the one_hot encoding is no longer used, but the ordinary encoding is used.

The third problem：
After the previous changes, the inference function starts to work normally, but the value of loss is very large, the resulting sequence has a high error rate, and the attention map is irregular.After repeatedly tuning various hyperparameters, the loss is finally reduced to a satisfactory level.

At this time, I was surprised to find that the attention map generated by the test file was almost the same as expected.

As shown in the figure:

![ZLYHIK3A$ GI15U`$1O{4FU](https://user-images.githubusercontent.com/91429283/163824184-df112278-97f9-4fc7-88bb-66115d40de96.png)

![9V5`}8%FL(XOP3 )0_RA7A4](https://user-images.githubusercontent.com/91429283/163824282-b52da081-abd2-4c13-851f-f4b80688426f.png)

![Y%NAUG 2SMV0FJ9{ZQJE AO](https://user-images.githubusercontent.com/91429283/163824287-3e56f979-a6ad-4406-9a3b-447130617638.png)
